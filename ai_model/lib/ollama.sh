#!/bin/bash
#
# ollama.sh - Ollama configuration and service management for setup-local-llm.sh
#
# Depends on: constants.sh, logger.sh, ui.sh, hardware.sh

# Setup Ollama environment variables for Apple Silicon optimization
setup_ollama_environment() {
  print_header "âš™ï¸ Configuring Ollama for Apple Silicon"
  
  local ollama_env_dir="$HOME/.ollama"
  local ollama_env_file="$ollama_env_dir/ollama.env"
  
  mkdir -p "$ollama_env_dir"
  
  # Calculate optimal number of threads (cores - 2, minimum 2)
  local optimal_threads=$((CPU_CORES - 2))
  if [[ $optimal_threads -lt 2 ]]; then
    optimal_threads=2
  fi
  
  # Set OLLAMA_NUM_GPU (1 for Metal GPU, -1 for auto)
  export OLLAMA_NUM_GPU=1
  
  # Set optimal thread count
  export OLLAMA_NUM_THREAD=$optimal_threads
  
  # Set keep-alive based on hardware tier
  case "$HARDWARE_TIER" in
    S) export OLLAMA_KEEP_ALIVE="1h" ;;
    A) export OLLAMA_KEEP_ALIVE="1h" ;;
    B) export OLLAMA_KEEP_ALIVE="5m" ;;
    C) export OLLAMA_KEEP_ALIVE="5m" ;;
    *) export OLLAMA_KEEP_ALIVE="5m" ;;
  esac
  
  # Set max loaded models: Always 3 total (1 embedding + 2 coding models)
  # This ensures nomic-embed-text is always available for code indexing
  export OLLAMA_MAX_LOADED_MODELS=3
  
  # Write environment variables to file for persistence
  cat > "$ollama_env_file" <<EOF
# Ollama Environment Variables for Apple Silicon Optimization
# Generated by setup-local-llm.sh
export OLLAMA_NUM_GPU=$OLLAMA_NUM_GPU
export OLLAMA_NUM_THREAD=$OLLAMA_NUM_THREAD
export OLLAMA_KEEP_ALIVE="$OLLAMA_KEEP_ALIVE"
export OLLAMA_MAX_LOADED_MODELS=$OLLAMA_MAX_LOADED_MODELS
EOF
  
  print_success "Ollama environment configured"
  print_info "  GPU: $OLLAMA_NUM_GPU (Metal)"
  print_info "  Threads: $OLLAMA_NUM_THREAD"
  print_info "  Keep-alive: $OLLAMA_KEEP_ALIVE"
  print_info "  Max loaded models: $OLLAMA_MAX_LOADED_MODELS"
  log_info "Ollama environment configured: GPU=$OLLAMA_NUM_GPU, Threads=$OLLAMA_NUM_THREAD"
}

# Configure Metal GPU acceleration
configure_metal_acceleration() {
  print_header "ðŸŽ® Configuring Metal GPU Acceleration"
  
  # Check if Metal framework is available (macOS 10.13+)
  local macos_version major_version minor_version
  macos_version=$(sw_vers -productVersion 2>/dev/null || echo "0.0.0")
  IFS='.' read -r major_version minor_version _ <<< "$macos_version"
  major_version=${major_version:-0}
  minor_version=${minor_version:-0}
  
  if [[ $major_version -lt 10 ]] || [[ $major_version -eq 10 && $minor_version -lt 13 ]]; then
    log_warn "Metal requires macOS 10.13+. Detected: $macos_version"
    return 1
  fi
  
  # Verify Metal is available
  if system_profiler SPDisplaysDataType 2>/dev/null | grep -q "Metal"; then
    print_success "Metal framework detected"
    log_info "Metal GPU acceleration available"
  else
    log_warn "Metal framework not detected, but continuing (may use CPU fallback)"
  fi
  
  # Ensure environment variables are set
  setup_ollama_environment
  
  print_success "Metal GPU acceleration configured"
}

# Verify Metal GPU usage
verify_metal_usage() {
  print_info "Verifying Metal GPU acceleration..."
  log_info "Verifying Metal GPU usage"
  
  # Wait a moment for Ollama to be ready
  sleep 2
  
  # Check if Ollama API is responding
  if ! curl -s http://localhost:11434/api/tags &>/dev/null; then
    log_warn "Ollama API not responding, cannot verify GPU usage"
    return 1
  fi
  
  # Try to get process info from Ollama API
  local ps_response=$(curl -s http://localhost:11434/api/ps 2>/dev/null || echo "")
  
  if [[ -n "$ps_response" ]]; then
    # Check if response contains GPU-related information
    if echo "$ps_response" | grep -qi "gpu\|metal\|device"; then
      print_success "GPU acceleration appears to be active"
      log_info "Metal GPU verification: Active"
      return 0
    fi
  fi
  
  # Alternative: Check Ollama logs or process info
  # On Apple Silicon, Ollama should automatically use Metal if available
  print_info "Metal GPU should be active (Ollama auto-detects on Apple Silicon)"
  log_info "Metal GPU verification: Assumed active (auto-detection)"
  return 0
}

# Get optimized model name
# Note: Ollama automatically selects optimal quantization (Q4_K_M/Q5_K_M) for Apple Silicon
# We don't need to specify quantization in the model name - Ollama handles it automatically
get_optimized_model_name() {
  local base_model="$1"
  
  # Ollama automatically downloads the best quantization for your system
  # On Apple Silicon, it will use Q4_K_M or Q5_K_M automatically
  # Just return the base model name - Ollama handles optimization
  echo "$base_model"
}

# Configure Ollama service with environment variables
configure_ollama_service() {
  print_info "Configuring Ollama service..."
  log_info "Configuring Ollama service with environment variables"
  
  # Source environment variables
  local ollama_env_file="$HOME/.ollama/ollama.env"
  if [[ -f "$ollama_env_file" ]]; then
    # Source the environment file
    set -a
    source "$ollama_env_file" 2>/dev/null || true
    set +a
    log_info "Loaded Ollama environment from $ollama_env_file"
  fi
  
  # Check if Ollama is running via brew services
  if brew services list 2>/dev/null | grep -q "ollama.*started"; then
    print_info "Restarting Ollama service to apply optimizations..."
    brew services restart ollama 2>/dev/null || true
    sleep 3
    log_info "Ollama service restarted"
  else
    # If not using brew services, start manually with environment
    if ! curl -s http://localhost:11434/api/tags &>/dev/null; then
      print_info "Starting Ollama with optimized environment..."
      # Start Ollama in background with environment variables
      (
        if [[ -f "$ollama_env_file" ]]; then
          source "$ollama_env_file"
        fi
        ollama serve > /dev/null 2>&1 &
      )
      sleep 3
      log_info "Ollama started with environment variables"
    fi
  fi
}

# Get list of installed models (cached to avoid multiple calls)
get_installed_models() {
  # Use a global cache variable to avoid multiple ollama list calls
  if [[ -z "${_INSTALLED_MODELS_CACHE:-}" ]]; then
    # Check if Ollama is available before calling
    if ! command -v ollama &>/dev/null; then
      echo ""
      return 1
    fi
    
    # Check if Ollama service is running
    if ! curl -s --max-time 2 http://localhost:11434/api/tags &>/dev/null; then
      log_warn "Ollama service not running, cannot get installed models"
      echo ""
      return 1
    fi
    
    _INSTALLED_MODELS_CACHE=$(ollama list 2>/dev/null | tail -n +2 | awk '{print $1}' || echo "")
  fi
  echo "$_INSTALLED_MODELS_CACHE"
}

# Clear installed models cache (call after installing/uninstalling models)
clear_installed_models_cache() {
  unset _INSTALLED_MODELS_CACHE
  log_info "Cleared installed models cache"
}

# Verify model installation via Ollama API (more reliable than CLI)
# Returns 0 if model is found, 1 otherwise
verify_model_via_api() {
  local model="$1"
  local model_base="${model%%:*}"
  local model_tag="${model#*:}"
  
  # If no tag specified, model_tag equals model_base
  if [[ "$model_tag" == "$model_base" ]]; then
    model_tag="latest"
  fi
  
  # Query the Ollama API for installed models
  local api_response
  api_response=$(curl -s --max-time 10 http://localhost:11434/api/tags 2>/dev/null)
  
  if [[ -z "$api_response" ]]; then
    log_warn "Ollama API not responding for model verification"
    return 1
  fi
  
  # Check if jq is available for proper JSON parsing
  if command -v jq &>/dev/null; then
    # Use jq for reliable JSON parsing
    # Check for exact model match first
    if echo "$api_response" | jq -e ".models[]? | select(.name == \"$model\")" &>/dev/null; then
      log_info "Model $model verified via API (exact match)"
      return 0
    fi
    
    # Check for model:latest if no tag was originally specified
    if echo "$api_response" | jq -e ".models[]? | select(.name == \"${model_base}:${model_tag}\")" &>/dev/null; then
      log_info "Model ${model_base}:${model_tag} verified via API"
      return 0
    fi
    
    # Check for any version of this model (base name match)
    if echo "$api_response" | jq -e ".models[]? | select(.name | startswith(\"${model_base}:\"))" &>/dev/null; then
      log_info "Model with base name $model_base verified via API"
      return 0
    fi
  else
    # Fallback: grep-based parsing (less reliable but works without jq)
    # Look for the model name in the JSON response
    if echo "$api_response" | grep -q "\"name\":\"${model}\""; then
      log_info "Model $model verified via API (grep match)"
      return 0
    fi
    
    if echo "$api_response" | grep -q "\"name\":\"${model_base}:${model_tag}\""; then
      log_info "Model ${model_base}:${model_tag} verified via API (grep match)"
      return 0
    fi
    
    # Check for any version with this base name
    if echo "$api_response" | grep -qE "\"name\":\"${model_base}:[^\"]+\""; then
      log_info "Model with base name $model_base verified via API (grep match)"
      return 0
    fi
  fi
  
  log_info "Model $model not found via API verification"
  return 1
}

# Wait for model to appear in Ollama with progressive backoff
# This handles the delay between download completion and model registration
wait_for_model_registration() {
  local model="$1"
  local max_wait="${2:-30}"  # Maximum seconds to wait
  local waited=0
  local check_interval=2
  
  log_info "Waiting for model $model to be registered (max ${max_wait}s)..."
  
  while [[ $waited -lt $max_wait ]]; do
    # Clear cache before each check
    clear_installed_models_cache
    
    # Try API verification first (more reliable)
    if verify_model_via_api "$model"; then
      log_info "Model $model registered after ${waited}s"
      return 0
    fi
    
    # Fallback to ollama show
    if ollama show "$model" &>/dev/null; then
      log_info "Model $model confirmed via ollama show after ${waited}s"
      return 0
    fi
    
    sleep "$check_interval"
    waited=$((waited + check_interval))
    
    # Increase check interval progressively (but cap at 5s)
    if [[ $check_interval -lt 5 ]]; then
      check_interval=$((check_interval + 1))
    fi
  done
  
  log_warn "Model $model not registered after ${max_wait}s wait"
  return 1
}

# Check if a specific model is installed
is_model_installed() {
  local model="$1"
  local installed_models
  installed_models=$(get_installed_models)
  
  # Extract base name and tag from the requested model
  local model_base="${model%%:*}"
  local model_tag="${model#*:}"
  # If no tag was specified, model_tag equals model_base
  if [[ "$model_tag" == "$model_base" ]]; then
    model_tag=""
  fi
  
  # Check for exact match first (highest priority)
  if echo "$installed_models" | grep -qxF "$model"; then
    return 0
  fi
  
  # Check for model with :latest tag (Ollama often returns models this way)
  # Only if user didn't specify a tag, or specified :latest
  if [[ -z "$model_tag" ]] || [[ "$model_tag" == "latest" ]]; then
    if echo "$installed_models" | grep -qxF "${model_base}:latest"; then
      return 0
    fi
  fi
  
  # Check if model name matches with any tag - but use EXACT base name matching
  # Use word boundaries to avoid llama3 matching llama3.1
  # The pattern ensures we match the exact base name followed by a colon and tag
  if [[ -z "$model_tag" ]]; then
    # User requested base model without tag - check if ANY version is installed
    # Use fixed string match for base name, then check for colon
    while IFS= read -r installed; do
      if [[ -n "$installed" ]]; then
        local installed_base="${installed%%:*}"
        # Exact match of base name (not prefix match)
        if [[ "$installed_base" == "$model_base" ]]; then
          return 0
        fi
      fi
    done <<< "$installed_models"
  fi
  
  # Fallback: try using ollama show which is more reliable
  # This handles edge cases where the model list might not be up to date
  if ollama show "$model" &>/dev/null; then
    return 0
  fi
  
  # Also try with :latest suffix if no tag was specified
  if [[ -z "$model_tag" ]]; then
    if ollama show "${model_base}:latest" &>/dev/null; then
      return 0
    fi
  fi
  
  return 1
}

# macOS-compatible timeout wrapper
# Tries timeout, gtimeout, or falls back to background process with kill
run_with_timeout() {
  local timeout_seconds="$1"
  shift
  local cmd=("$@")
  local response_file=$(mktemp)
  local pid
  local exit_code=0
  
  # Try timeout command first (GNU coreutils)
  if command -v timeout &>/dev/null; then
    if timeout "$timeout_seconds" "${cmd[@]}" > "$response_file" 2>&1; then
      cat "$response_file"
      rm -f "$response_file"
      return 0
    else
      exit_code=$?
      cat "$response_file"
      rm -f "$response_file"
      return $exit_code
    fi
  # Try gtimeout (GNU coreutils via Homebrew)
  elif command -v gtimeout &>/dev/null; then
    if gtimeout "$timeout_seconds" "${cmd[@]}" > "$response_file" 2>&1; then
      cat "$response_file"
      rm -f "$response_file"
      return 0
    else
      exit_code=$?
      cat "$response_file"
      rm -f "$response_file"
      return $exit_code
    fi
  # Fallback: background process with kill
  else
    "${cmd[@]}" > "$response_file" 2>&1 &
    pid=$!
    local waited=0
    while kill -0 "$pid" 2>/dev/null && [[ $waited -lt $timeout_seconds ]]; do
      sleep 1
      waited=$((waited + 1))
    done
    
    if kill -0 "$pid" 2>/dev/null; then
      # Process still running after timeout
      kill -TERM "$pid" 2>/dev/null || true
      sleep 1
      kill -KILL "$pid" 2>/dev/null || true
      wait "$pid" 2>/dev/null || true
      cat "$response_file"
      rm -f "$response_file"
      return 124  # Timeout exit code (matches GNU timeout)
    else
      # Process completed
      wait "$pid" 2>/dev/null
      exit_code=$?
      cat "$response_file"
      rm -f "$response_file"
      return $exit_code
    fi
  fi
}

# Unload model from memory using Ollama API
unload_model() {
  local model="$1"
  local silent="${2:-0}"  # Optional: 1 for silent mode
  
  # Check if model is actually loaded
  if ! ollama ps 2>/dev/null | grep -q "^${model}"; then
    return 0  # Model not loaded, nothing to do
  fi
  
  if [[ $silent -eq 0 ]]; then
    print_info "Unloading model from memory..."
  fi
  
  # Use Ollama API to unload the model by setting keep_alive to 0
  # This is the proper way to unload a model according to Ollama docs
  local api_response
  api_response=$(curl -s --max-time 10 -X POST http://localhost:11434/api/generate \
    -H "Content-Type: application/json" \
    -d "{\"model\": \"$model\", \"prompt\": \"\", \"keep_alive\": 0}" 2>/dev/null || echo "")
  
  # Wait for the model to unload (Ollama needs a moment)
  sleep 3
  
  # Retry if still loaded (sometimes takes a moment)
  local retries=0
  while [[ $retries -lt 3 ]] && ollama ps 2>/dev/null | grep -q "^${model}"; do
    sleep 1
    ((retries++))
    # Try again with a minimal request
    curl -s --max-time 5 -X POST http://localhost:11434/api/generate \
      -H "Content-Type: application/json" \
      -d "{\"model\": \"$model\", \"prompt\": \"\", \"keep_alive\": 0}" >/dev/null 2>&1 || true
  done
  
  # Verify model is unloaded
  if ollama ps 2>/dev/null | grep -q "^${model}"; then
    if [[ $silent -eq 0 ]]; then
      print_warn "Model may still be in memory"
    fi
    return 1
  else
    if [[ $silent -eq 0 ]]; then
      print_success "Model unloaded from memory"
    fi
    return 0
  fi
}

# Unload all models from memory (cleanup function)
unload_all_models() {
  local loaded_models
  loaded_models=$(ollama ps 2>/dev/null | tail -n +2 | awk '{print $1}' || echo "")
  
  if [[ -z "$loaded_models" ]]; then
    return 0  # No models loaded
  fi
  
  print_info "Unloading all models from memory..."
  
  while IFS= read -r model; do
    if [[ -n "$model" ]]; then
      unload_model "$model" 1  # Silent mode
    fi
  done <<< "$loaded_models"
  
  # Final check
  sleep 2
  local remaining
  remaining=$(ollama ps 2>/dev/null | tail -n +2 | wc -l | xargs || echo "0")
  if [[ $remaining -eq 0 ]]; then
    print_success "All models unloaded"
  else
    print_warn "$remaining model(s) may still be in memory"
  fi
}
