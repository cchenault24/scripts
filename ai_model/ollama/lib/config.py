"""
Continue.dev configuration generation.

Provides functions to generate Continue.dev config.yaml and config.json files.
Supports both old ModelInfo and new RecommendedModel types.

Also handles:
- Installation manifest creation for smart uninstallation
- File fingerprinting for tracking generated files
"""

import hashlib
import json
import shutil
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from . import hardware
from . import ui

# Version for fingerprinting
INSTALLER_VERSION = "2.0.0"

# Fingerprint header for generated files
FINGERPRINT_COMMENT = f"# Generated by ollama-llm-setup.py v{INSTALLER_VERSION}"


def _normalize_model(model: Any) -> Dict[str, Any]:
    """
    Normalize a model to a common dictionary format.
    
    Supports:
    - models.ModelInfo (old format)
    - model_selector.RecommendedModel (new format)
    
    Returns:
        Dictionary with normalized model info
    """
    # Try to import RecommendedModel type
    try:
        from .model_selector import RecommendedModel
        if isinstance(model, RecommendedModel):
            return {
                "name": model.name,
                "ollama_name": model.ollama_name,
                "ram_gb": model.ram_gb,
                "roles": model.roles,
                "context_length": 32768,  # Default for RecommendedModel
            }
    except ImportError:
        pass
    
    # Handle old ModelInfo format
    if hasattr(model, 'ollama_name'):
        return {
            "name": model.name,
            "ollama_name": model.ollama_name,
            "ram_gb": model.ram_gb,
            "roles": getattr(model, 'roles', ["chat"]),
            "context_length": getattr(model, 'context_length', 32768),
        }
    
    # Handle dictionary format
    if isinstance(model, dict):
        return model
    
    raise ValueError(f"Unknown model type: {type(model)}")


def format_yaml_value(value: Any) -> str:
    """Format a value for YAML output."""
    if isinstance(value, bool):
        return "true" if value else "false"
    if isinstance(value, str):
        # Quote strings with special characters
        if any(c in value for c in ":#{}[]&*!|>'\"%@`"):
            return f'"{value}"'
        return value
    return str(value)


def generate_yaml(config: Dict[str, Any], indent: int = 0) -> str:
    """Generate YAML string from config dict."""
    lines = []
    prefix = "  " * indent
    
    for key, value in config.items():
        if key.startswith("#"):
            lines.append(f"{prefix}{key}")
            continue
        
        if value is None:
            continue
        
        if isinstance(value, dict):
            lines.append(f"{prefix}{key}:")
            lines.append(generate_yaml(value, indent + 1))
        elif isinstance(value, list):
            lines.append(f"{prefix}{key}:")
            for item in value:
                if isinstance(item, dict):
                    # First item on same line with dash
                    first = True
                    for k, v in item.items():
                        if first:
                            if isinstance(v, list):
                                lines.append(f"{prefix}  - {k}:")
                                for vi in v:
                                    lines.append(f"{prefix}      - {vi}")
                            else:
                                lines.append(f"{prefix}  - {k}: {format_yaml_value(v)}")
                            first = False
                        else:
                            if isinstance(v, list):
                                lines.append(f"{prefix}    {k}:")
                                for vi in v:
                                    lines.append(f"{prefix}      - {vi}")
                            else:
                                lines.append(f"{prefix}    {k}: {format_yaml_value(v)}")
                else:
                    lines.append(f"{prefix}  - {format_yaml_value(item)}")
        else:
            lines.append(f"{prefix}{key}: {format_yaml_value(value)}")
    
    return "\n".join(lines)


def generate_setup_summary(
    model_list: List[Any],
    hw_info: hardware.HardwareInfo
) -> Dict[str, Any]:
    """
    Generate setup summary with hardware tier, models, and RAM usage.
    
    Args:
        model_list: List of selected models (ModelInfo or RecommendedModel)
        hw_info: Hardware information
    
    Returns:
        Dictionary with setup summary
    """
    # Normalize models
    normalized_models = [_normalize_model(m) for m in model_list]
    
    total_ram_used = sum(m["ram_gb"] for m in normalized_models)
    usable_ram = hw_info.get_estimated_model_memory()
    reserve_ram = usable_ram - total_ram_used
    
    models_summary = []
    for model in normalized_models:
        models_summary.append({
            "name": model["name"],
            "ollama_name": model["ollama_name"],
            "variant": "default",
            "ram_gb": model["ram_gb"],
            "roles": model["roles"],
            "context_length": model["context_length"]
        })
    
    summary = {
        "hardware": {
            "tier": hw_info.tier.value,
            "ram_gb": hw_info.ram_gb,
            "usable_ram_gb": usable_ram,
            "cpu": hw_info.cpu_brand or "Unknown",
            "apple_chip": hw_info.apple_chip_model or None,
            "has_apple_silicon": hw_info.has_apple_silicon
        },
        "models": models_summary,
        "ram_usage": {
            "total_ram_gb": total_ram_used,
            "available_ram_gb": usable_ram,
            "reserve_ram_gb": reserve_ram,
            "usage_percent": (total_ram_used / usable_ram * 100) if usable_ram > 0 else 0
        },
        "timestamp": str(Path.home() / ".continue" / "setup-summary.json")
    }
    
    return summary


def save_setup_summary(
    model_list: List[Any],
    hw_info: hardware.HardwareInfo,
    output_path: Optional[Path] = None
) -> Path:
    """
    Save setup summary to JSON file.
    
    Args:
        model_list: List of selected models
        hw_info: Hardware information
        output_path: Optional output path (default: ~/.continue/setup-summary.json)
    
    Returns:
        Path to saved summary file
    """
    if output_path is None:
        output_path = Path.home() / ".continue" / "setup-summary.json"
    
    # Create directory if needed
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Generate summary
    summary = generate_setup_summary(model_list, hw_info)
    summary["timestamp"] = str(output_path)
    
    # Save to file
    with open(output_path, "w") as f:
        json.dump(summary, f, indent=2)
    
    ui.print_success(f"Setup summary saved to {output_path}")
    
    return output_path


def generate_continue_config(
    model_list: List[Any],
    hw_info: hardware.HardwareInfo,
    output_path: Optional[Path] = None,
    target_ide: Optional[List[str]] = None
) -> Path:
    """
    Generate continue.dev config files.
    
    Args:
        model_list: List of selected models (ModelInfo or RecommendedModel)
        hw_info: Hardware information
        output_path: Optional output path (default: ~/.continue/config.yaml)
        target_ide: List of IDEs to configure (e.g., ["vscode"], ["intellij"], or ["vscode", "intellij"])
                   Defaults to ["vscode"] for backward compatibility.
                   If "intellij" only, skips YAML generation (IntelliJ doesn't use YAML).
    
    Returns:
        Path to saved config file (YAML if VS Code, JSON if IntelliJ only)
    """
    # Input validation
    if not model_list:
        raise ValueError("model_list cannot be empty")
    if not hw_info:
        raise ValueError("hw_info is required")
    
    # Normalize all models
    normalized_models = [_normalize_model(m) for m in model_list]
    
    # Default to VS Code for backward compatibility
    if target_ide is None:
        target_ide = ["vscode"]
    
    # Determine if we should generate YAML (VS Code uses YAML, IntelliJ doesn't)
    generate_yaml = "vscode" in target_ide
    
    ui.print_header("ðŸ“ Generating Continue.dev Configuration")
    
    # Determine output path based on target IDE
    if output_path is None:
        if generate_yaml:
            output_path = Path.home() / ".continue" / "config.yaml"
        else:
            # IntelliJ only - use JSON path
            output_path = Path.home() / ".continue" / "config.json"
    
    # Validate output path
    if output_path.parent and not output_path.parent.exists():
        try:
            output_path.parent.mkdir(parents=True, exist_ok=True)
        except (OSError, PermissionError) as e:
            raise ValueError(f"Cannot create config directory: {e}") from e
    
    # Create directory if needed
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Backup existing config if present
    if output_path.exists():
        backup_path = output_path.with_suffix(".yaml.backup")
        shutil.copy(output_path, backup_path)
        ui.print_info(f"Backed up existing config to {backup_path}")
    
    # Use the detected API endpoint from hardware info
    api_base = hw_info.ollama_api_endpoint
    ui.print_info(f"Using API endpoint: {api_base}")
    
    # Ensure apiBase doesn't have trailing slash and includes /v1 for OpenAI-compatible API
    # Continue.dev expects the full base URL including /v1 for OpenAI-compatible APIs
    api_base_clean = api_base.rstrip('/')
    # Ensure it includes /v1 if not already present
    if '/v1' not in api_base_clean:
        # If the endpoint doesn't have /v1, we need to determine the correct base
        # For Ollama, the API is typically at /v1
        if api_base_clean.endswith(':11434') or api_base_clean.endswith(':8080'):
            api_base_clean = f"{api_base_clean}/v1"
        elif 'localhost' in api_base_clean:
            api_base_clean = f"{api_base_clean}/v1" if not api_base_clean.endswith('/v1') else api_base_clean
    
    # Build config with comments
    # Note: 'name' and 'version' are REQUIRED fields per the Continue.dev config schema (config-yaml-schema.json)
    # The 'schema' field is optional and not included here
    yaml_lines = [
        "# Continue.dev Configuration for Ollama",
        "# Generated by ollama-llm-setup.py",
        f"# Hardware: {hw_info.apple_chip_model or hw_info.cpu_brand}",
        f"# RAM: {hw_info.ram_gb:.0f}GB | Tier: {hw_info.tier.value}",
        "#",
        "# Documentation: https://docs.continue.dev/yaml-reference",
        "",
        "name: Ollama Local LLM",
        "version: 1.0.0",
        "",
    ]
    
    # Find models by role (using normalized models)
    chat_models = [m for m in normalized_models if "chat" in m["roles"] or "edit" in m["roles"]]
    autocomplete_models = [m for m in normalized_models if "autocomplete" in m["roles"]]
    embed_models = [m for m in normalized_models if "embed" in m["roles"]]
    
    # Sort chat models by RAM (largest first = highest quality)
    chat_models.sort(key=lambda m: m["ram_gb"], reverse=True)
    
    # Sort autocomplete models by RAM (smallest first = fastest)
    autocomplete_models.sort(key=lambda m: m["ram_gb"])
    
    # Build models section
    # YAML indentation rules:
    # - Top-level keys: no indentation
    # - List items under a key: 2 spaces then dash
    # - Properties of list items: 4 spaces
    # - Nested list items: 6 spaces then dash
    # - Nested properties: 6 spaces
    yaml_lines.append("models:")
    
    for i, model in enumerate(chat_models):
        model_id = model["ollama_name"]  # Use ollama_name directly
        # Model list item: 2 spaces before dash, 4 spaces for properties
        yaml_lines.extend([
            f"  - name: {model['name']}",
            f"    provider: openai",
            f"    model: {model_id}",
            f"    apiBase: {api_base_clean}",
        ])
        
        # Add roles (only valid roles per schema: chat, autocomplete, embed, rerank, edit, apply, summarize)
        roles = ["chat", "edit", "apply"]
        # Note: "agent" is not in the schema's role enum, so we don't include it
        if "autocomplete" in model["roles"]:
            roles.append("autocomplete")
        if "embed" in model["roles"]:
            roles.append("embed")
        # Roles property: 4 spaces, nested list items: 6 spaces before dash
        yaml_lines.append("    roles:")
        for role in roles:
            yaml_lines.append(f"      - {role}")
        
        # Add autocompleteOptions if model has autocomplete role
        if "autocomplete" in model["roles"]:
            # autocompleteOptions property: 4 spaces, nested properties: 6 spaces
            yaml_lines.extend([
                "    autocompleteOptions:",
                "      debounceDelay: 300",
                "      modelTimeout: 3000",
            ])
        
        # Add defaultCompletionOptions with contextLength
        # defaultCompletionOptions property: 4 spaces, nested properties: 6 spaces
        yaml_lines.extend([
            "    defaultCompletionOptions:",
            f"      contextLength: {model['context_length']}",
        ])
        
        # Note: supportsToolCalls is not in the official schema and causes validation errors
        # Continue.dev will handle @codebase with system message tools automatically
        yaml_lines.append("")
    
    # Add autocomplete models (with autocompleteOptions)
    # Same indentation rules: 2 spaces before dash, 4 spaces for properties, 6 spaces for nested items
    chat_ollama_names = {m["ollama_name"] for m in chat_models}
    autocomplete_only = [m for m in autocomplete_models if m["ollama_name"] not in chat_ollama_names]
    for model in autocomplete_only:
        model_id = model["ollama_name"]
        yaml_lines.extend([
            f"  - name: {model['name']} (Autocomplete)",
            f"    provider: openai",
            f"    model: {model_id}",
            f"    apiBase: {api_base_clean}",
            "    roles:",
            "      - autocomplete",
            "    autocompleteOptions:",
            "      debounceDelay: 300",
            "      modelTimeout: 3000",
            "    defaultCompletionOptions:",
            f"      contextLength: {model['context_length']}",
            "",
        ])
    
    # Add embedding models (if different from chat and autocomplete models)
    # Same indentation rules: 2 spaces before dash, 4 spaces for properties, 6 spaces for nested items
    auto_ollama_names = {m["ollama_name"] for m in autocomplete_models}
    embed_only = [m for m in embed_models if m["ollama_name"] not in chat_ollama_names and m["ollama_name"] not in auto_ollama_names]
    for model in embed_only:
        model_id = model["ollama_name"]
        yaml_lines.extend([
            f"  - name: {model['name']} (Embedding)",
            f"    provider: openai",
            f"    model: {model_id}",
            f"    apiBase: {api_base_clean}",
            "    roles:",
            "      - embed",
            "    defaultCompletionOptions:",
            f"      contextLength: {model['context_length']}",
            "",
        ])
    
    # Context providers (using new schema format)
    yaml_lines.extend([
        "# Context providers for code understanding",
        "# Note: contextProviders is deprecated, use 'context' instead",
        "context:",
        "  - provider: codebase",
        "  - provider: folder",
        "  - provider: file",
        "  - provider: code",
        "  - provider: terminal",
        "  - provider: diff",
        "  - provider: problems",
        "  - provider: open",
        "",
    ])
    
    # Note: 'experimental' and 'ui' are NOT in the official schema and cause validation errors
    # The schema has additionalProperties: false, so only explicitly defined properties are allowed
    # These settings were removed to comply with the schema validation
    
    # Add Apple Silicon specific optimizations
    if hw_info.has_apple_silicon:
        yaml_lines.extend([
            f"# Optimized for {hw_info.apple_chip_model or 'Apple Silicon'}",
            f"# Available unified memory: ~{hw_info.get_estimated_model_memory():.0f}GB",
            "# Metal GPU acceleration is enabled automatically",
            f"# Neural Engine: {hw_info.neural_engine_cores} cores available",
            "# Apple Silicon optimizations:",
            "#   - Metal Performance Shaders (MPS) for GPU acceleration",
            "#   - Unified memory architecture (shared CPU/GPU/NE memory)",
            "#   - Optimized quantization for Apple Silicon",
        ])
    
    # Write YAML config (only if VS Code is in target_ide)
    if generate_yaml:
        yaml_content = "\n".join(yaml_lines)
        
        with open(output_path, "w") as f:
            f.write(yaml_content)
        
        ui.print_success(f"Configuration saved to {output_path}")
    
    # Always create JSON version (works for both VS Code and IntelliJ)
    if generate_yaml:
        json_path = output_path.parent / "config.json"
    else:
        # IntelliJ only - use the output_path for JSON
        json_path = output_path
    
    # Build JSON config (using new schema format)
    # Note: 'name' and 'version' are REQUIRED fields per the Continue.dev config schema (config-yaml-schema.json)
    json_config: Dict[str, Any] = {
        "name": "Ollama Local LLM",
        "version": "1.0.0",
        "models": []
    }
    
    for model in chat_models:
        model_id = model["ollama_name"]
        # Only valid roles per schema: chat, autocomplete, embed, rerank, edit, apply, summarize
        # Note: "agent" is not in the schema's role enum, so we don't include it
        roles = ["chat", "edit", "apply"]
        if "autocomplete" in model["roles"]:
            roles.append("autocomplete")
        if "embed" in model["roles"]:
            roles.append("embed")
        
        model_config = {
            "name": model["name"],
            "provider": "openai",
            "model": model_id,
            "apiBase": api_base_clean,
            "defaultCompletionOptions": {
                "contextLength": model["context_length"],
            },
            "roles": roles,
        }
        
        # Add autocompleteOptions if model has autocomplete role
        if "autocomplete" in model["roles"]:
            model_config["autocompleteOptions"] = {
                "debounceDelay": 300,
                "modelTimeout": 3000,
            }
        
        json_config["models"].append(model_config)
    
    for model in autocomplete_only:
        model_id = model["ollama_name"]
        json_config["models"].append({
            "name": f"{model['name']} (Autocomplete)",
            "provider": "openai", 
            "model": model_id,
            "apiBase": api_base_clean,
            "roles": ["autocomplete"],
            "autocompleteOptions": {
                "debounceDelay": 300,
                "modelTimeout": 3000,
            },
            "defaultCompletionOptions": {
                "contextLength": model["context_length"],
            },
        })
    
    # Add embedding models (already filtered above in YAML section)
    for model in embed_only:
        model_id = model["ollama_name"]
        json_config["models"].append({
            "name": f"{model['name']} (Embedding)",
            "provider": "openai",
            "model": model_id,
            "apiBase": api_base_clean,
            "roles": ["embed"],
            "defaultCompletionOptions": {
                "contextLength": model["context_length"],
            },
        })
    
    # Context providers (new format)
    json_config["context"] = [
        {"provider": "codebase"},
        {"provider": "folder"},
        {"provider": "file"},
        {"provider": "code"},
        {"provider": "terminal"},
        {"provider": "diff"},
        {"provider": "problems"},
        {"provider": "open"},
    ]
    
    # Note: 'experimental' and 'ui' are NOT in the official schema and cause validation errors
    # The schema has additionalProperties: false, so only explicitly defined properties are allowed
    # These settings were removed to comply with the schema validation
    
    with open(json_path, "w") as f:
        json.dump(json_config, f, indent=2)
    
    if generate_yaml:
        ui.print_info(f"JSON config also saved to {json_path}")
    else:
        ui.print_success(f"Configuration saved to {json_path}")
        # Return JSON path for IntelliJ-only case
        return json_path
    
    return output_path


def generate_continueignore(
    output_path: Optional[Path] = None
) -> Path:
    """
    Generate .continueignore file to ignore non-source code files and directories.
    
    Args:
        output_path: Optional output path (default: ~/.continue/.continueignore)
    
    Returns:
        Path to saved .continueignore file
    """
    if output_path is None:
        # Default to ~/.continue/.continueignore (Continue.dev configuration directory)
        output_path = Path.home() / ".continue" / ".continueignore"
    
    # Create directory if needed
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Backup existing .continueignore if present
    if output_path.exists():
        backup_path = output_path.with_suffix(".continueignore.backup")
        shutil.copy(output_path, backup_path)
        ui.print_info(f"Backed up existing .continueignore to {backup_path}")
    
    # Generate .continueignore content
    ignore_content = """# Version control
.git/
.svn/
.hg/
.gitignore
.gitattributes

# Python cache and compiled files
__pycache__/
*.py[cod]
*$py.class
*.so
*.egg
*.egg-info/
dist/
dist
build/
*.whl
.python-version
*.pyc
*.pyo
*.pyd

# Node.js and JavaScript
node_modules/
node_modules
bower_components
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*
.yarn/
.pnp.*
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.yarn-dev-pid
.pnp.js
.next/
.nuxt/
.output/
.turbo/
.parcel-cache/
.cache/
.eslintcache
.stylelintcache

# Java and JVM languages
target/
.gradle/
.gradle_home
.mvn/
*.class
*.jar
*.war
*.ear
*.nar
*.hprof

# Go
vendor/
*.exe
*.exe~
*.dll
*.so
*.dylib

# Rust
target/
Cargo.lock

# PHP
vendor/
composer.lock

# Ruby
vendor/bundle/
vendor/bundle
.bundle/
*.gem
*.rbc
.byebug_history

# Virtual environments
venv/
.venv/
env/
ENV/
.ENV/
.conda/

# IDE and editor files
.vscode/
.idea/
**/.idea
.theia
*.swp
*.swo
*~
.project
*.project
.pydevproject
*.iml
*.classpath
*.classpath.txt
.settings/
.metadata
.recommenders
*.sublime-project
*.sublime-workspace
*.code-workspace

# OS files
.DS_Store
*.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db
Desktop.ini
$RECYCLE.BIN/

# Testing and coverage
.coverage
.coverage.*
htmlcov/
.pytest_cache/
.tox/
.hypothesis/
.nyc_output/
coverage/
*.lcov
.jest/
reports
screenshots
public/screenshots
task-*.json

# Logs and temporary files
*.log
*.tmp
*.temp
*.bak
*.backup
*.hash
tmp/
temp/
temp/*
.tmp/
/log/*
/tmp/*
/tmp/pids/*

# Documentation builds
docs/_build/
site/
_book/
.docusaurus/

# Jupyter Notebook
.ipynb_checkpoints
*.ipynb_checkpoints

# Environment variables
.env
.env.bundle
.env.local
.env.*.local
.envrc

# Build and output directories
out/
output/
bin/
obj/
lib/
libs/
*.a
*.o
*.dylib
*.dll
public/assets
buildinfo

# Package manager locks and caches
package-lock.json
yarn.lock
pnpm-lock.yaml
composer.lock
Pipfile.lock
poetry.lock
Gemfile.lock
Podfile.lock

# Framework-specific
.sass-cache/
.angular/
.vuepress/dist/
.serverless/
.aws-sam/
.terraform/
.terraform.lock.hcl
.terraform.tfstate*
terraform.tfstate*
*.tfstate
*.tfstate.*
*.tfvars
vars.json

# Database files
*.db
*.sqlite
*.sqlite3
*.db-journal
database.*
database1.*
db/*.sqlite3
db/*.sqlite3-journal
db/development.*
db/*.zip
*.trace.db

# Compiled assets
*.min.js
*.min.css
*.css
*.html
*.js
*.map
assets/dist/
public/dist/
static/dist/

# Container and VM files
.vagrant
.containerid
.pongo/.bash_history

# Generated and cache files
.repository/.cache/*
**/*.lastUpdated
luacov.stats.out
luacov.report.out
servroot

# Certificates and keys
*.pfx
*.crt
*.key
certificates/
keystores/
ssl/

# Miscellaneous
audio
images
backup
help
tags
"""
    
    # Write .continueignore
    with open(output_path, "w") as f:
        f.write(ignore_content)
    
    ui.print_success(f".continueignore file saved to {output_path}")
    
    return output_path


def generate_global_rule(
    output_path: Optional[Path] = None
) -> Path:
    """
    Generate Continue.dev global-rule.md file.
    
    Args:
        output_path: Optional output path (default: ~/.continue/rules/global-rule.md)
    
    Returns:
        Path to saved global-rule.md file
    """
    if output_path is None:
        output_path = Path.home() / ".continue" / "rules" / "global-rule.md"
    
    # Create directory if needed
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Backup existing rule if present
    if output_path.exists():
        backup_path = output_path.with_suffix(".md.backup")
        shutil.copy(output_path, backup_path)
        ui.print_info(f"Backed up existing global-rule.md to {backup_path}")
    
    # Generate global-rule.md content
    rule_content = """---
description: Senior-level coding assistant rules for React, TypeScript, Redux, and Material-UI projects
---

CRITICAL RESPONSE RULE: 
- General questions (definitions, explanations, concepts) â†’ Answer ONLY with clear English text
- Code requests (implementation, debugging, refactoring) â†’ Provide code with explanation
- When in doubt, default to English explanation first, then ask if code is needed

DO NOT write code for questions like "What is X?", "How does Y work?", "Explain Z"
ONLY write code when explicitly asked to implement, fix, or create something.

Output Format:
- Respond directly to the question asked
- Do not prefix responses with role labels, agent names, or tool descriptions
- Do not include metadata like "Agent:", "Tool:", "Creating file:", etc.
- Match response type to question type (English for questions, code for implementations)

Act as a senior-level coding assistant with deep expertise in React, TypeScript, Redux, and Material-UI. Provide guidance that reflects industry best practices and production-ready code quality.

This project uses React, TypeScript, Redux, and Material-UI. Always provide context-aware assistance based on these technologies.

Context Handling: Include project-wide context for architectural decisions and patterns. Keep context focused for specific coding tasks to balance comprehensiveness with performance.

Code Review: Apply strict standards by default - flag potential issues, enforce best practices, and point out style inconsistencies. Be adaptive for quick fixes or exploratory work where appropriate. Review code with the rigor expected at senior engineer level.

TypeScript: Balance type safety with practicality. Use type inference where clear, explicit types where needed. Match existing type patterns in the codebase. Avoid `any` type - prefer `unknown` for type-safe alternatives. Use `never` appropriately for exhaustiveness checking and functions that never return. Demonstrate senior-level TypeScript expertise including advanced types when appropriate.

Redux: Always match the existing Redux patterns used in the codebase. Do not suggest alternative patterns. Apply best practices for state management, action creators, and selectors.

Material-UI: Match the existing Material-UI version and component patterns currently used in the codebase. Do not suggest upgrades or newer approaches.

Code Formatting: Always suggest proper formatting based on Prettier/ESLint rules configured in the project.

Dependencies: Prefer native/built-in solutions over external dependencies when reasonable. When external dependencies are needed, match the project's existing dependency patterns. Consider long-term maintenance implications.

Code Review Priority: When reviewing code, prioritize in this order: 1) Security vulnerabilities and bugs, 2) Performance implications, 3) Maintainability and readability, 4) Accessibility compliance. Identify issues a senior engineer would catch.

React Components: Use functional components with hooks for all new code. When reviewing existing class components, only suggest refactoring if explicitly asked to modernize the code or if the class component has hooks-related bugs or limitations. Otherwise, work within the existing component pattern.

Error Handling: Point out missing error handling and edge cases, but don't automatically add it unless explicitly asked. Match the error handling patterns already established in the codebase. Consider production-level error scenarios.

Refactoring: Briefly mention improvement opportunities while staying focused on the main question. Suggest improvements for code quality and bugs, but not for style or preference changes. Think about scalability and maintainability.

Verbosity: Be concise for simple tasks and detailed for complex architectural decisions. Adapt explanation depth based on the complexity of the request. Communicate with the clarity expected between senior engineers.

Testing: Assume testing is handled separately. Do not mention testing considerations unless explicitly asked.

Code Quality: Provide production-ready code that considers edge cases, performance, accessibility, and long-term maintainability. Avoid overly clever solutions in favor of clear, maintainable code.
"""
    
    # Write global-rule.md
    with open(output_path, "w") as f:
        f.write(rule_content)
    
    ui.print_success(f"Global rule file saved to {output_path}")
    
    return output_path


# =============================================================================
# Manifest and Fingerprinting Functions
# =============================================================================

def calculate_file_hash(filepath: Path) -> str:
    """Calculate SHA-256 hash of a file for fingerprinting."""
    if not filepath.exists():
        return ""
    
    try:
        with open(filepath, 'rb') as f:
            return hashlib.sha256(f.read()).hexdigest()
    except (OSError, IOError, PermissionError):
        # File access issues - return empty hash
        return ""


def classify_file_type(filepath: Path) -> str:
    """Classify a file by its type for manifest."""
    suffix = filepath.suffix.lower()
    name = filepath.name.lower()
    
    if suffix in ('.yaml', '.yml'):
        return 'config_yaml'
    elif suffix == '.json':
        if 'manifest' in name:
            return 'manifest'
        elif 'summary' in name:
            return 'summary'
        return 'config_json'
    elif suffix == '.md':
        return 'rule'
    elif name == '.continueignore':
        return 'ignore'
    elif suffix == '.backup':
        return 'backup'
    else:
        return 'other'


def add_fingerprint_header(content: str, file_type: str) -> str:
    """
    Add fingerprint header to generated files.
    
    Args:
        content: The file content
        file_type: Type of file (yaml, md, json, etc.)
    
    Returns:
        Content with fingerprint header
    """
    timestamp = datetime.now().isoformat()
    
    if file_type in ("yaml", "yml", "md"):
        header = f"""{FINGERPRINT_COMMENT}
# Timestamp: {timestamp}
# DO NOT EDIT MANUALLY - This file is managed by ollama-llm-setup
# To customize, use the setup script's customization options

"""
        return header + content
    
    # For JSON, we add metadata to the content object instead
    # (handled separately in JSON generation)
    return content


def create_installation_manifest(
    installed_models: List[Any],
    created_files: List[Path],
    hw_info: 'hardware.HardwareInfo',
    target_ide: List[str],
    pre_existing_models: List[str]
) -> Path:
    """
    Create manifest of what was installed for uninstaller.
    
    Args:
        installed_models: List of models that were successfully pulled
        created_files: List of files created during setup
        hw_info: Hardware information
        target_ide: List of target IDEs
        pre_existing_models: Models that existed before setup
    
    Returns:
        Path to the created manifest file
    """
    # Normalize models
    normalized_models = []
    for m in installed_models:
        model_dict = _normalize_model(m)
        normalized_models.append({
            "name": model_dict["ollama_name"],
            "display_name": model_dict["name"],
            "size_gb": model_dict["ram_gb"],
            "pulled_at": datetime.now().isoformat(),
            "roles": model_dict["roles"]
        })
    
    # Create file entries with fingerprints
    file_entries = []
    for f in created_files:
        if f.exists():
            file_entries.append({
                "path": str(f),
                "fingerprint": calculate_file_hash(f),
                "type": classify_file_type(f),
                "created_at": datetime.now().isoformat()
            })
    
    # Backup file paths
    continue_dir = Path.home() / ".continue"
    backup_files = {
        "config_yaml": str(continue_dir / "config.yaml.backup"),
        "config_json": str(continue_dir / "config.json.backup"),
        "global_rule": str(continue_dir / "rules" / "global-rule.md.backup")
    }
    
    # Check which backups actually exist
    existing_backups = {
        k: v for k, v in backup_files.items() 
        if Path(v).exists()
    }
    
    manifest = {
        "version": "2.0",
        "timestamp": datetime.now().isoformat(),
        "installer_version": INSTALLER_VERSION,
        "hardware_snapshot": {
            "tier": hw_info.tier.value,
            "ram_gb": hw_info.ram_gb,
            "cpu": hw_info.cpu_brand or "Unknown",
            "apple_chip": hw_info.apple_chip_model,
            "has_apple_silicon": hw_info.has_apple_silicon
        },
        "pre_existing": {
            "models": pre_existing_models,
            "backups": existing_backups
        },
        "installed": {
            "models": normalized_models,
            "files": file_entries,
            "cache_dirs": [str(continue_dir / "cache")],
            "ide_extensions": [f"{ide}:Continue.continue" for ide in target_ide],
            "ollama_available": getattr(hw_info, 'ollama_available', False),
            "target_ides": target_ide
        }
    }
    
    manifest_path = continue_dir / "setup-manifest.json"
    
    # Ensure directory exists
    manifest_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=2)
    
    ui.print_success(f"Installation manifest saved to {manifest_path}")
    
    return manifest_path


def load_installation_manifest() -> Optional[Dict[str, Any]]:
    """
    Load the installation manifest if it exists.
    
    Returns:
        Manifest dictionary or None if not found
    """
    manifest_path = Path.home() / ".continue" / "setup-manifest.json"
    
    if not manifest_path.exists():
        return None
    
    try:
        with open(manifest_path, 'r') as f:
            return json.load(f)
    except (OSError, IOError, json.JSONDecodeError, PermissionError):
        # File access or parsing issues - no manifest available
        return None


def is_our_file(filepath: Path, manifest: Optional[Dict[str, Any]] = None) -> Union[bool, str]:
    """
    Check if file was created by our installer.
    
    Args:
        filepath: Path to check
        manifest: Optional manifest for comparison
    
    Returns:
        True if definitely ours, False if not, "maybe" if uncertain
    """
    if not filepath.exists():
        return False
    
    # Check 1: In manifest?
    if manifest:
        manifest_files = manifest.get("installed", {}).get("files", [])
        for entry in manifest_files:
            if entry.get("path") == str(filepath):
                # Verify fingerprint still matches
                current_hash = calculate_file_hash(filepath)
                if current_hash == entry.get("fingerprint"):
                    return True
                else:
                    return "maybe"  # In manifest but modified
    
    # Check 2: Has our fingerprint header?
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            first_lines = "".join([f.readline() for _ in range(5)])
            if "Generated by ollama-llm-setup.py" in first_lines:
                return True
    except (OSError, IOError, UnicodeDecodeError, PermissionError):
        # File access or encoding issues
        pass
    
    # Check 3: Timestamp during installation window?
    if manifest:
        try:
            install_time = datetime.fromisoformat(manifest.get("timestamp", ""))
            file_mtime = datetime.fromtimestamp(filepath.stat().st_mtime)
            
            # Within 1 hour of installation
            if abs((file_mtime - install_time).total_seconds()) < 3600:
                return "maybe"  # Uncertain - ask user
        except (ValueError, OSError, TypeError):
            # Invalid timestamp format or file stat issues
            pass
    
    return False


def check_config_customization(config_path: Path, manifest: Optional[Dict[str, Any]] = None) -> str:
    """
    Check if user customized config file.
    
    Args:
        config_path: Path to config file
        manifest: Optional manifest for comparison
    
    Returns:
        Status: "missing", "unknown", "modified", "unchanged"
    """
    if not config_path.exists():
        return "missing"
    
    if not manifest:
        # No manifest - check for fingerprint
        status = is_our_file(config_path)
        if status is True:
            return "unchanged"
        elif status == "maybe":
            return "modified"
        return "unknown"
    
    # Find in manifest
    config_entry = None
    for file_entry in manifest.get("installed", {}).get("files", []):
        if file_entry.get("path") == str(config_path):
            config_entry = file_entry
            break
    
    if not config_entry:
        return "unknown"  # Not in manifest
    
    # Check fingerprint
    current_hash = calculate_file_hash(config_path)
    if current_hash != config_entry.get("fingerprint"):
        return "modified"  # User customized it
    
    return "unchanged"
